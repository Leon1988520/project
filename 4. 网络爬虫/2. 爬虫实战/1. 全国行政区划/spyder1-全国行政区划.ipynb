{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取全国行政区划\n",
    "\n",
    "\n",
    "1. 网站地址[http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2017/index.html](http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2017/index.html)\n",
    "\n",
    "\n",
    "## 爬取策略\n",
    "1. 爬取主页数据，将链接数据入队\n",
    "2. 在队列中，依次拿出各网址，爬取该网址的数据，入队\n",
    "3. 每一页中，逐行将链接的文字，放入数据库中\n",
    "\n",
    "\n",
    "## 知识点\n",
    "1. requests\n",
    "2. beautifulsoup\n",
    "    - soup.select('tr[class=\"villagetr\"]')\n",
    "    - soup.find_all(\"a\")\n",
    "\n",
    "3. 爬虫攻防：构建头\n",
    "\n",
    "\n",
    "## 参考\n",
    "1. [python简单爬虫](http://cache.baiducontent.com/c?m=9d78d513d9810ae902b0c8690d67c0171e43f1612ba7d10208d08448e2320c1e1a72a4fb792d4a4295873d7000dc5441afb57365377471ebcb96d51f9cac925f7ed57829234cd11f539404edd64126c327975ce9b81990e0b66dcd&p=b4769a4786cc4ae000a48e2c4f&newp=82769a47928911a053a4d6275953d8224216ed623fd4c44324b9d71fd325001c1b69e7bc2d261702d4c4796d0bad4d5aeef63078341766dada9fca458ae7c46c65&user=baidu&fm=sc&query=python+%C5%C0%B3%E6+demo&qid=bd8c9e6500034ce3&p1=2)\n",
    "2. [1个小白五小时的爬虫经历](https://www.cnblogs.com/panzi/p/6421826.html)\n",
    "3. [32个爬虫项目-让你一次吃到饱](https://blog.csdn.net/qq_41396296/article/details/79428834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入库\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "dir_base = \"http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"STYLE3\" href=\"http://www.miibeian.gov.cn/\" target=\"_blank\">京ICP备05034670号</a>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得html文档\n",
    "def get_html(url):\n",
    "    \"\"\"get the content of the url\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    header = {\n",
    "#         \"User-Agent\":\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36\"\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'\n",
    "        ,'Referer':'http://www.stats.gov.cn/'\n",
    "    }\n",
    "    res = session.get(url, headers=header, timeout = 50000)\n",
    "    res.encoding = \"gb2312\"\n",
    "    return BeautifulSoup(res.text)\n",
    "\n",
    "soup = get_html(\"%s/%s\" %(dir_base, \"62/09/22/620922215.html\"))\n",
    "soup.select(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "常顺村委会\n",
      "沙河村委会\n",
      "临河村委会\n",
      "民和村委会\n",
      "河洲村委会\n"
     ]
    }
   ],
   "source": [
    "for link in soup.select('tr[class=\"villagetr\"]'):\n",
    "    print(link.find_all(\"td\")[2].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file D:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 地址格式(五级)\n",
    "# 省 市 区/县 乡镇/街道 居委会\n",
    "\"\"\"\n",
    "算法：\n",
    "    1. 创建一个行标，一个空数组，用于存放行政区划\n",
    "    2. 从根路径index开始，采用深度优先搜索策略\n",
    "        爬取当前页面，如果当前页面有链接，则压入栈内\n",
    "                      如果当前页面没有链接(除了外链)，即已经爬到了末端，将该页面的内容写入行政区划表，row+1\n",
    "        取栈中的下一个地址\n",
    "            如果栈中记录的不是地址，那么row-1，并在对应列中写入栈中记入的数据，row+1\n",
    "            取栈中的下一个地址\n",
    "    3. 空值填充，后向填充\n",
    "\"\"\"\n",
    "row = 0\n",
    "df_district = pd.DataFrame()\n",
    "\n",
    "html_list = [{\"url\":\"%s/%s\" %(dir_base, \"index.html\")}]\n",
    "\n",
    "f = open(r\"E:\\workspace\\0. git\\project\\4. 网络爬虫\\2. 爬虫实战\\1. 全国行政区划\\addr.txt\", \"w\")\n",
    "\n",
    "while len(html_list) > 0:\n",
    "    time.sleep(random.random())   # 休眠随机秒(0-1秒)\n",
    "    node = html_list.pop()\n",
    "    if \"url\" in list(node.keys()):\n",
    "        url = node[\"url\"]\n",
    "    else:\n",
    "        key_no = node[\"key_no\"]\n",
    "        value_no = node[\"value_no\"]\n",
    "        tag = node[\"tag\"]\n",
    "        value_tag = node[\"value_tag\"]\n",
    "        \n",
    "        row -= 1\n",
    "        df_district.loc[row, key_no] = value_no\n",
    "        df_district.loc[row, tag] = value_tag\n",
    "        row += 1\n",
    "        continue\n",
    "    \n",
    "    soup = get_html(url)\n",
    "    # 如果找到a\n",
    "    if len(soup.find_all(\"a\")) == 1: # 如果找到的a标签只有1个，即[<a class=\"STYLE3\" href=\"http://www.miibeian.gov.cn/\" target=\"_blank\">京ICP备05034670号</a>]，表示爬到了末端\n",
    "        for link in soup.select('tr[class=\"villagetr\"]'):\n",
    "            df_district.loc[row, \"v_no\"] = link.find_all(\"td\")[1].get_text()\n",
    "            df_district.loc[row, \"village\"] = link.find_all(\"td\")[2].get_text()\n",
    "            \n",
    "#             df_district.loc[row, \"t_no\"] = link.find_all(\"td\")[1].get_text()\n",
    "#             df_district.loc[row, \"town\"] = link.find_all(\"td\")[2].get_text()\n",
    "            \n",
    "            row += 1\n",
    "        continue\n",
    "        \n",
    "    for link in soup.find_all(\"a\"):\n",
    "        html = link.get(\"href\")\n",
    "        tag = link.get_text()\n",
    "        \n",
    "        if html.startswith(\"http\") == False and re.match(r\"\\d+\", tag) is None:\n",
    "            if len(html) == 7:\n",
    "                html_list.append({\"tag\":\"province\", \"value_tag\":tag, \"key_no\":\"p_no\", \"value_no\":html[:2]})\n",
    "                r = \"%s/%s\" %(dir_base, html)\n",
    "            elif len(html) == 12:\n",
    "                html_list.append({\"tag\":\"city\", \"value_tag\":tag, \"key_no\":\"c_no\", \"value_no\":html[3:7]})\n",
    "                r = \"%s/%s\" %(dir_base, html)\n",
    "            elif len(html) == 14:\n",
    "                html_list.append({\"tag\":\"district\", \"value_tag\":tag, \"key_no\":\"d_no\", \"value_no\":html[3:9]})\n",
    "                r = \"%s/%s/%s\" %(dir_base, html[3:5], html)\n",
    "            elif len(html) == 17:\n",
    "                html_list.append({\"tag\":\"town\", \"value_tag\":tag, \"key_no\":\"t_no\", \"value_no\":html[3:12]})\n",
    "                r = \"%s/%s/%s/%s\" %(dir_base, html[3:5], html[5:7], html)\n",
    "                \n",
    "            html_list.append({\"url\":r})\n",
    "            f.write(\"%s\\n\" %(r))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_district.fillna(method=\"backfill\", inplace=True)\n",
    "\n",
    "df_district.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(html_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_district.to_csv(r\"C:\\Users\\Progress\\Desktop\\district.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
